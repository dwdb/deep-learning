{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "biaffine_attention_dependency_parser.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "160pqa4QGgpevhgy9UvdAc1DObSp2FP-H",
      "authorship_tag": "ABX9TyPmqVIE/gg/DblD2mADwBS8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dwdb/dependency-parser/blob/master/biaffine_attention_dependency_parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-UXV_Scnk4n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "b7318707-9032-4197-fc8e-f8cf7b59a869"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from collections import defaultdict\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jun 21 13:56:45 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyVH9wU_nvzt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Token(object):\n",
        "    def __init__(self, token_id, word, pos, rel=-1, head_id=-1):\n",
        "        self.token_id = token_id\n",
        "        self.word = word\n",
        "        self.pos = pos\n",
        "        self.rel = rel\n",
        "        self.head_id = head_id\n",
        "\n",
        "\n",
        "class ConllLoader(object):\n",
        "    def fit_transform(self, path, min_count=2, max_len=128):\n",
        "        dataset = self.load(path)\n",
        "\n",
        "        # build dictionary\n",
        "        word_count = defaultdict(int)\n",
        "        self.pos_dict = {'<pad>': 0, '<unk>': 1, '<root>': 2}\n",
        "        self.rel_dict = {'<pad>': 0, '<unk>': 1, '<root>': 2}\n",
        "        for sentence in dataset:\n",
        "            for token in sentence:\n",
        "                word_count[token.word] += 1\n",
        "                self.pos_dict.setdefault(token.pos, len(self.pos_dict))\n",
        "                self.rel_dict.setdefault(token.rel, len(self.rel_dict))\n",
        "        self.word_dict = {'<pad>': 0, '<root>': 1, '<unk>': 2}\n",
        "        for word, count in word_count.items():\n",
        "            if count >= min_count:\n",
        "                self.word_dict.setdefault(word, len(self.word_dict))\n",
        "        self.id2word, _ = zip(*sorted(self.word_dict.items(), key=lambda x:x[1]))\n",
        "        self.id2pos, _ = zip(*sorted(self.pos_dict.items(), key=lambda x:x[1]))\n",
        "        self.id2rel, _ = zip(*sorted(self.rel_dict.items(), key=lambda x:x[1]))\n",
        "        # transform\n",
        "        return self._transform(dataset, max_len)\n",
        "\n",
        "    def _transform(self, dataset, max_len=None):\n",
        "        inputs = []\n",
        "        for sentence in dataset:\n",
        "            sentence_input = []\n",
        "            for token in sentence:\n",
        "                word_id = self.word_dict.get(token.word,  self.word_dict['<unk>'])\n",
        "                pos_id = self.pos_dict.get(token.pos, self.pos_dict['<unk>'])\n",
        "                head_id =  token.head_id\n",
        "                rel_id = self.rel_dict.get(token.rel, self.rel_dict['<unk>'])\n",
        "                sentence_input.append((word_id, pos_id, head_id, rel_id))\n",
        "            inputs.append(list(zip(*sentence_input)) + [len(sentence)])\n",
        "            sentence_input= []\n",
        "        words, poss, heads, rels, seq_lens = zip(*inputs)\n",
        "        words = self.pad_sequences(words, max_len=max_len)\n",
        "        poss = self.pad_sequences(poss, max_len=max_len)\n",
        "        heads = self.pad_sequences(heads, max_len=max_len)\n",
        "        rels = self.pad_sequences(rels, max_len=max_len)\n",
        "        seq_lens = tf.convert_to_tensor(seq_lens, tf.int32)\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((words, poss, heads, rels, seq_lens))\n",
        "        return dataset\n",
        "\n",
        "    def transform(self, path, max_len=128):\n",
        "        return self._transform(self.load(path), max_len=max_len)\n",
        "    \n",
        "    def pad_sequences(self, sequences, max_len=None):\n",
        "        return tf.keras.preprocessing.sequence.pad_sequences(\n",
        "            sequences, max_len, padding='post', truncating='post', value=0)\n",
        "\n",
        "    @staticmethod\n",
        "    def load(path, max_len=128):\n",
        "        dataset = []\n",
        "        root_token = Token(0, '<root>', '<root>', '<root>', 0)\n",
        "        with open(path, encoding='utf8') as f:\n",
        "            sentence = [root_token]\n",
        "            for line in f.readlines():\n",
        "                if line == '\\n' or line.startswith('#'):\n",
        "                    if 1 < len(sentence) <= max_len:\n",
        "                        dataset.append(sentence)\n",
        "                    sentence = [root_token]\n",
        "                    continue\n",
        "                line = line.split('\\t')\n",
        "                token = Token(int(line[0]), line[1], line[3], line[7], int(line[6]))\n",
        "                sentence.append(token)\n",
        "        return dataset\n",
        "\n",
        "\n",
        "conll_loader = ConllLoader()\n",
        "conll_path = '/content/drive/My Drive/dependency parsing/data/conll'\n",
        "train_dataset = conll_loader.fit_transform(\n",
        "    path=os.path.join(conll_path, 'train.conll'), max_len=128, min_count=2)\n",
        "valid_dataset = conll_loader.transform(\n",
        "    path=os.path.join(conll_path, 'dev.conll'), max_len=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4S-afgjjkjt6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "db51e18a-22a7-4373-8856-7371d195063a"
      },
      "source": [
        " class MLP(tf.keras.layers.Layer):\n",
        "    def __init__(self, units, num_layers=1, dropout_rate=0.):\n",
        "        super(MLP, self).__init__()\n",
        "        self.denses = []\n",
        "        for _ in range(num_layers):\n",
        "            self.denses.append(tf.keras.layers.Dropout(dropout_rate))\n",
        "            self.denses.append(tf.keras.layers.Dense(units))\n",
        "    \n",
        "    def call(self, x, training=False):\n",
        "        for layer in self.denses:\n",
        "            x = layer(x, training=training)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Biaffine(tf.keras.layers.Layer):\n",
        "    def __init__(self, in_size, out_size, bias_x=False, bias_y=False):\n",
        "        super(Biaffine, self).__init__()\n",
        "        self.bias_x = bias_x\n",
        "        self.bias_y = bias_y\n",
        "        self.w = self.add_weight(\n",
        "            name='weight', \n",
        "            shape=(out_size, in_size + int(bias_x), in_size + int(bias_y)),\n",
        "            trainable=True)\n",
        "        \n",
        "    def call(self, input1, input2):\n",
        "        if self.bias_x:\n",
        "            input1 = tf.concat((input1, tf.ones_like(input1[..., :1])), axis=-1)\n",
        "        if self.bias_y:\n",
        "            input2 = tf.concat((input2, tf.ones_like(input2[..., :1])), axis=-1)\n",
        "        # bxi,oij,byj->boxy\n",
        "        logits = tf.einsum('bxi,oij,byj->boxy', input1, self.w, input2)\n",
        "        return logits\n",
        "\n",
        "\n",
        "class BiaffineAttentionModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, pos_size, embedding_size, num_lstm_units, num_lstm_layers,\n",
        "                 num_mlt_layers, arc_mlt_size, rel_mlt_size, rel_size, dropout_rate):\n",
        "        super(BiaffineAttentionModel, self).__init__()\n",
        "\n",
        "        self.word_embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.pos_embedding = tf.keras.layers.Embedding(pos_size, embedding_size)\n",
        "\n",
        "        self.lstms = [tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(\n",
        "            num_lstm_units, return_sequences=True)) for _ in range(num_lstm_layers)]\n",
        "        \n",
        "        self.arc_head = MLP(arc_mlt_size, num_mlt_layers, dropout_rate)\n",
        "        self.arc_dep = MLP(arc_mlt_size, num_mlt_layers, dropout_rate)\n",
        "        self.rel_head = MLP(rel_mlt_size, num_mlt_layers, dropout_rate)\n",
        "        self.rel_dep = MLP(rel_mlt_size, num_mlt_layers, dropout_rate)\n",
        " \n",
        "        self.arc_biaffine = Biaffine(arc_mlt_size, 1, bias_x=True, bias_y=False)\n",
        "        self.rel_biaffine = Biaffine(rel_mlt_size, rel_size, bias_x=True, bias_y=True)\n",
        "\n",
        "        self.embedding_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, word_ids, pos_ids, training=False):\n",
        "        word_embedding = self.word_embedding(word_ids)\n",
        "        pos_embedding = self.pos_embedding(pos_ids)\n",
        "        embedding = tf.concat((word_embedding, pos_embedding), axis=-1)\n",
        "        self.embedding_dropout(embedding, training=training)\n",
        "        # bilstm layer\n",
        "        lstm_output = embedding\n",
        "        for lstm in self.lstms:\n",
        "            lstm_output = lstm(lstm_output)\n",
        "        # mlt and biaffine layer\n",
        "        # shape=(batch_size, 1, seq_len, seq_len)\n",
        "        arc_logit = self.arc_biaffine(\n",
        "            self.arc_dep(lstm_output, training), self.arc_head(lstm_output, training))\n",
        "        arc_logit = tf.squeeze(arc_logit, axis=1)\n",
        "        # shape=(batch_size, rel_size, seq_len, seq_len)\n",
        "        rel_logit = self.rel_biaffine(\n",
        "            self.rel_dep(lstm_output, training), self.rel_head(lstm_output, training))\n",
        "        rel_logit = tf.transpose(rel_logit, perm=(0, 2, 3 ,1))\n",
        "        return arc_logit, rel_logit\n",
        "\n",
        "\n",
        "class BiaffineAttention(object):\n",
        "    def __init__(self, vocab_size, pos_size, embedding_size, num_lstm_units, num_lstm_layers,\n",
        "                 num_mlt_layers, arc_mlt_size, rel_mlt_size, rel_size, learning_rate, adam_beta_2, \n",
        "                 dropout_rate):\n",
        "        self.model = BiaffineAttentionModel(\n",
        "            vocab_size=vocab_size,\n",
        "            pos_size=pos_size,\n",
        "            embedding_size=embedding_size,\n",
        "            num_lstm_units=num_lstm_units,\n",
        "            num_lstm_layers=num_lstm_layers,\n",
        "            num_mlt_layers=num_mlt_layers,\n",
        "            arc_mlt_size=arc_mlt_size,\n",
        "            rel_mlt_size=rel_mlt_size,\n",
        "            rel_size=rel_size,\n",
        "            dropout_rate=dropout_rate)\n",
        "        self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy(True, reduction='none')\n",
        "\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate, beta_2=adam_beta_2)\n",
        "        self.global_step = tf.Variable(0, trainable=False, name='globle_step')\n",
        "\n",
        "        self.metric_loss = tf.keras.metrics.Mean(name='loss')\n",
        "        self.metric_uas = tf.keras.metrics.Mean(name='uas')\n",
        "        self.metric_las = tf.keras.metrics.Mean(name='las')\n",
        "\n",
        "    @staticmethod\n",
        "    def get_rel_indices(arc_true):\n",
        "        batch_size = tf.shape(arc_true)[0]\n",
        "        seq_len = tf.shape(arc_true)[1]\n",
        "        index1 = tf.tile(tf.expand_dims(tf.range(batch_size), -1), [1, seq_len])\n",
        "        index2 = tf.tile(tf.expand_dims(tf.range(seq_len), 0), [batch_size, 1])\n",
        "        indices = tf.stack((index1, index2, arc_true), axis=2)\n",
        "        return indices\n",
        "    \n",
        "    def loss_function(self, arc_true, arc_pred, rel_true, rel_pred, mask):\n",
        "        arc_loss = self.loss_object(arc_true, arc_pred)\n",
        "        rel_loss = self.loss_object(rel_true, rel_pred)\n",
        "        # ignore arc and rel of the root word\n",
        "        loss = tf.boolean_mask((arc_loss + rel_loss)[:, 1:], mask[:, 1:])\n",
        "        loss = tf.reduce_mean(loss)\n",
        "        return loss\n",
        "\n",
        "    def get_uas_las(self, arc_true, arc_pred, rel_true, rel_pred, mask):\n",
        "        # calculate uas\n",
        "        arc_pred = tf.argmax(arc_pred, -1, output_type=arc_true.dtype)\n",
        "        arc_pred = tf.boolean_mask(arc_pred[:, 1:], mask[:, 1:])\n",
        "        arc_true = tf.boolean_mask(arc_true[:, 1:], mask[:, 1:])\n",
        "        arc_correct = arc_true == arc_pred\n",
        "        uas = tf.reduce_mean(tf.cast(arc_correct, tf.float32))\n",
        "        # calculate las\n",
        "        rel_pred = tf.argmax(rel_pred, -1, output_type=rel_true.dtype)\n",
        "        rel_pred = tf.boolean_mask(rel_pred[:, 1:], mask[:, 1:])\n",
        "        rel_true = tf.boolean_mask(rel_true[:, 1:], mask[:, 1:])\n",
        "        rel_correct = tf.logical_and(rel_true == rel_pred, arc_correct)\n",
        "        las = tf.reduce_mean(tf.cast(rel_correct, tf.float32))\n",
        "        return uas, las\n",
        "\n",
        "    def train(self, train_dataset, epochs, batch_size, valid_dataset=None):\n",
        "        assert isinstance(train_dataset, tf.data.Dataset), 'unknown dataset type!'\n",
        "        train_dataset = train_dataset.shuffle(20000).batch(batch_size)\n",
        "        time_record = time.time()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            for batch_data in train_dataset:\n",
        "                self.train_step(*batch_data)\n",
        "\n",
        "                if self.global_step % 50 == 0:\n",
        "                    print('epoch:%d step:%d sesc:%.2fs loss:%.4f uas:%.4f, las:%.4f' % (\n",
        "                        epoch, \n",
        "                        self.global_step.numpy(), \n",
        "                        time.time() - time_record, \n",
        "                        self.metric_loss.result().numpy(), \n",
        "                        self.metric_uas.result().numpy(),\n",
        "                        self.metric_las.result().numpy()\n",
        "                        ))\n",
        "                    self.metric_loss.reset_states()\n",
        "                    self.metric_uas.reset_states()\n",
        "                    self.metric_las.reset_states()\n",
        "                    time_record = time.time()\n",
        "\n",
        "            if valid_dataset is not None:\n",
        "                self.evaluate(valid_dataset)\n",
        "\n",
        "    @tf.function(input_signature=[\n",
        "        tf.TensorSpec(shape=(None, None), dtype=tf.int32), \n",
        "        tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
        "        tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
        "        tf.TensorSpec(shape=(None, None), dtype=tf.int32), \n",
        "        tf.TensorSpec(shape=(None, ), dtype=tf.int32)])\n",
        "    def train_step(self, input_word, input_pos, input_head, input_rel, input_len):\n",
        "        mask = tf.sequence_mask(input_len, tf.shape(input_word)[1])\n",
        "        with tf.GradientTape() as tape:\n",
        "            arc_logit, rel_logit = self.model(input_word, input_pos, training=True)\n",
        "            rel_logit = tf.gather_nd(rel_logit, self.get_rel_indices(input_head))\n",
        "            loss = self.loss_function(input_head, arc_logit, input_rel, rel_logit, mask)\n",
        "\n",
        "        # apply clipped gradients\n",
        "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
        "        clipped_grads = [tf.clip_by_value(grad, -2.0, 2.0) for grad in grads]\n",
        "        self.optimizer.apply_gradients(zip(clipped_grads, self.model.trainable_variables))\n",
        "\n",
        "        # calculate uas and las\n",
        "        uas, las = self.get_uas_las(input_head, arc_logit, input_rel, rel_logit, mask)\n",
        "\n",
        "        # update step\n",
        "        self.global_step.assign_add(1)\n",
        "\n",
        "        # record metric\n",
        "        self.metric_loss(loss)\n",
        "        self.metric_uas(uas)\n",
        "        self.metric_las(las)\n",
        "        return loss, uas, las\n",
        "    \n",
        "    def evaluate(self, dataset, batch_size=64):\n",
        "        UAS = LAS = 0.\n",
        "        steps = 0\n",
        "        for input_word, input_pos, input_head, input_rel, input_len in dataset.batch(batch_size):\n",
        "            arc_logit, rel_logit = self.model(input_word, input_pos, training=False)\n",
        "            rel_logit = tf.gather_nd(rel_logit, self.get_rel_indices(input_head))\n",
        "            # caculate uas and las\n",
        "            mask = tf.sequence_mask(input_len, tf.shape(input_word)[1])\n",
        "            uas, las = self.get_uas_las(input_head, arc_logit, input_rel, rel_logit, mask)\n",
        "            UAS, LAS, steps = UAS + uas.numpy(), LAS + las.numpy(), steps + 1\n",
        "        print('evaluation uas:%.4f las:%.4f' % (UAS / steps, LAS / steps))\n",
        "\n",
        "\n",
        "model = BiaffineAttention(\n",
        "    vocab_size=len(conll_loader.word_dict),\n",
        "    pos_size=len(conll_loader.pos_dict),\n",
        "    rel_size=len(conll_loader.rel_dict),\n",
        "    embedding_size=100,\n",
        "    num_lstm_units=400,\n",
        "    num_lstm_layers=3,\n",
        "    num_mlt_layers=1,\n",
        "    arc_mlt_size=500,\n",
        "    rel_mlt_size=100,\n",
        "    learning_rate=0.002,\n",
        "    adam_beta_2=0.9,\n",
        "    dropout_rate=0.33)\n",
        "model.train(train_dataset, epochs=15, batch_size=128, valid_dataset=valid_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:0 step:50 sesc:26.40s loss:7.3745 uas:0.0928, las:0.0185\n",
            "epoch:0 step:100 sesc:15.78s loss:2.8602 uas:0.4486, las:0.3346\n",
            "epoch:0 step:150 sesc:15.78s loss:1.3533 uas:0.6935, las:0.6474\n",
            "epoch:0 step:200 sesc:15.75s loss:0.9159 uas:0.7910, las:0.7568\n",
            "epoch:0 step:250 sesc:15.73s loss:0.7540 uas:0.8307, las:0.8014\n",
            "epoch:0 step:300 sesc:15.84s loss:0.6524 uas:0.8523, las:0.8271\n",
            "evaluation uas:0.8677 las:0.8445\n",
            "epoch:1 step:350 sesc:17.85s loss:0.5669 uas:0.8697, las:0.8466\n",
            "epoch:1 step:400 sesc:15.82s loss:0.5150 uas:0.8838, las:0.8617\n",
            "epoch:1 step:450 sesc:15.87s loss:0.4784 uas:0.8906, las:0.8712\n",
            "epoch:1 step:500 sesc:15.83s loss:0.4548 uas:0.8968, las:0.8770\n",
            "epoch:1 step:550 sesc:15.76s loss:0.4268 uas:0.9028, las:0.8853\n",
            "epoch:1 step:600 sesc:15.82s loss:0.4161 uas:0.9058, las:0.8881\n",
            "evaluation uas:0.9065 las:0.8880\n",
            "epoch:2 step:650 sesc:17.86s loss:0.3753 uas:0.9144, las:0.8981\n",
            "epoch:2 step:700 sesc:15.82s loss:0.3471 uas:0.9202, las:0.9039\n",
            "epoch:2 step:750 sesc:15.83s loss:0.3298 uas:0.9249, las:0.9093\n",
            "epoch:2 step:800 sesc:15.85s loss:0.3392 uas:0.9235, las:0.9079\n",
            "epoch:2 step:850 sesc:15.82s loss:0.3270 uas:0.9261, las:0.9108\n",
            "epoch:2 step:900 sesc:15.84s loss:0.3241 uas:0.9263, las:0.9114\n",
            "evaluation uas:0.9196 las:0.9042\n",
            "epoch:3 step:950 sesc:17.84s loss:0.2982 uas:0.9328, las:0.9180\n",
            "epoch:3 step:1000 sesc:15.85s loss:0.2547 uas:0.9410, las:0.9273\n",
            "epoch:3 step:1050 sesc:15.86s loss:0.2549 uas:0.9428, las:0.9288\n",
            "epoch:3 step:1100 sesc:15.85s loss:0.2606 uas:0.9397, las:0.9258\n",
            "epoch:3 step:1150 sesc:15.84s loss:0.2515 uas:0.9417, las:0.9282\n",
            "epoch:3 step:1200 sesc:15.81s loss:0.2584 uas:0.9412, las:0.9279\n",
            "evaluation uas:0.9244 las:0.9091\n",
            "epoch:4 step:1250 sesc:17.86s loss:0.2568 uas:0.9418, las:0.9284\n",
            "epoch:4 step:1300 sesc:15.83s loss:0.2037 uas:0.9528, las:0.9404\n",
            "epoch:4 step:1350 sesc:15.88s loss:0.1957 uas:0.9551, las:0.9426\n",
            "epoch:4 step:1400 sesc:15.88s loss:0.2076 uas:0.9531, las:0.9410\n",
            "epoch:4 step:1450 sesc:15.84s loss:0.2090 uas:0.9519, las:0.9394\n",
            "epoch:4 step:1500 sesc:15.84s loss:0.2037 uas:0.9534, las:0.9416\n",
            "epoch:4 step:1550 sesc:15.86s loss:0.2061 uas:0.9526, las:0.9404\n",
            "evaluation uas:0.9290 las:0.9148\n",
            "epoch:5 step:1600 sesc:17.91s loss:0.1637 uas:0.9615, las:0.9503\n",
            "epoch:5 step:1650 sesc:15.82s loss:0.1612 uas:0.9626, las:0.9518\n",
            "epoch:5 step:1700 sesc:15.84s loss:0.1576 uas:0.9632, las:0.9528\n",
            "epoch:5 step:1750 sesc:15.78s loss:0.1656 uas:0.9625, las:0.9510\n",
            "epoch:5 step:1800 sesc:15.82s loss:0.1649 uas:0.9620, las:0.9508\n",
            "epoch:5 step:1850 sesc:15.79s loss:0.1711 uas:0.9613, las:0.9501\n",
            "evaluation uas:0.9272 las:0.9125\n",
            "epoch:6 step:1900 sesc:17.86s loss:0.1425 uas:0.9671, las:0.9571\n",
            "epoch:6 step:1950 sesc:15.81s loss:0.1242 uas:0.9715, las:0.9616\n",
            "epoch:6 step:2000 sesc:15.78s loss:0.1260 uas:0.9703, las:0.9611\n",
            "epoch:6 step:2050 sesc:15.78s loss:0.1281 uas:0.9702, las:0.9607\n",
            "epoch:6 step:2100 sesc:15.78s loss:0.1328 uas:0.9691, las:0.9592\n",
            "epoch:6 step:2150 sesc:15.76s loss:0.1393 uas:0.9675, las:0.9570\n",
            "evaluation uas:0.9268 las:0.9124\n",
            "epoch:7 step:2200 sesc:17.87s loss:0.1263 uas:0.9705, las:0.9612\n",
            "epoch:7 step:2250 sesc:15.74s loss:0.1010 uas:0.9767, las:0.9685\n",
            "epoch:7 step:2300 sesc:15.78s loss:0.1038 uas:0.9762, las:0.9676\n",
            "epoch:7 step:2350 sesc:15.79s loss:0.1073 uas:0.9757, las:0.9669\n",
            "epoch:7 step:2400 sesc:15.82s loss:0.1064 uas:0.9752, las:0.9669\n",
            "epoch:7 step:2450 sesc:15.80s loss:0.1087 uas:0.9746, las:0.9660\n",
            "evaluation uas:0.9252 las:0.9105\n",
            "epoch:8 step:2500 sesc:17.86s loss:0.1118 uas:0.9740, las:0.9649\n",
            "epoch:8 step:2550 sesc:15.85s loss:0.0820 uas:0.9811, las:0.9744\n",
            "epoch:8 step:2600 sesc:15.85s loss:0.0858 uas:0.9801, las:0.9725\n",
            "epoch:8 step:2650 sesc:15.83s loss:0.0864 uas:0.9795, las:0.9724\n",
            "epoch:8 step:2700 sesc:15.82s loss:0.0872 uas:0.9796, las:0.9720\n",
            "epoch:8 step:2750 sesc:15.86s loss:0.0916 uas:0.9790, las:0.9714\n",
            "epoch:8 step:2800 sesc:15.86s loss:0.0924 uas:0.9791, las:0.9713\n",
            "evaluation uas:0.9274 las:0.9132\n",
            "epoch:9 step:2850 sesc:17.82s loss:0.0714 uas:0.9837, las:0.9775\n",
            "epoch:9 step:2900 sesc:15.84s loss:0.0726 uas:0.9829, las:0.9764\n",
            "epoch:9 step:2950 sesc:15.81s loss:0.0740 uas:0.9824, las:0.9762\n",
            "epoch:9 step:3000 sesc:15.78s loss:0.0741 uas:0.9832, las:0.9766\n",
            "epoch:9 step:3050 sesc:15.78s loss:0.0757 uas:0.9822, las:0.9756\n",
            "epoch:9 step:3100 sesc:15.76s loss:0.0792 uas:0.9817, las:0.9748\n",
            "evaluation uas:0.9276 las:0.9133\n",
            "epoch:10 step:3150 sesc:17.83s loss:0.0685 uas:0.9845, las:0.9783\n",
            "epoch:10 step:3200 sesc:15.85s loss:0.0623 uas:0.9855, las:0.9798\n",
            "epoch:10 step:3250 sesc:15.85s loss:0.0601 uas:0.9856, las:0.9803\n",
            "epoch:10 step:3300 sesc:15.82s loss:0.0653 uas:0.9848, las:0.9788\n",
            "epoch:10 step:3350 sesc:15.83s loss:0.0655 uas:0.9847, las:0.9791\n",
            "epoch:10 step:3400 sesc:15.86s loss:0.0678 uas:0.9844, las:0.9784\n",
            "evaluation uas:0.9273 las:0.9128\n",
            "epoch:11 step:3450 sesc:17.88s loss:0.0630 uas:0.9850, las:0.9796\n",
            "epoch:11 step:3500 sesc:15.83s loss:0.0538 uas:0.9878, las:0.9830\n",
            "epoch:11 step:3550 sesc:15.78s loss:0.0546 uas:0.9868, las:0.9819\n",
            "epoch:11 step:3600 sesc:15.82s loss:0.0540 uas:0.9875, las:0.9826\n",
            "epoch:11 step:3650 sesc:15.78s loss:0.0569 uas:0.9870, las:0.9818\n",
            "epoch:11 step:3700 sesc:15.80s loss:0.0560 uas:0.9873, las:0.9820\n",
            "evaluation uas:0.9251 las:0.9100\n",
            "epoch:12 step:3750 sesc:17.82s loss:0.0582 uas:0.9862, las:0.9810\n",
            "epoch:12 step:3800 sesc:15.82s loss:0.0439 uas:0.9901, las:0.9860\n",
            "epoch:12 step:3850 sesc:15.79s loss:0.0473 uas:0.9889, las:0.9847\n",
            "epoch:12 step:3900 sesc:15.83s loss:0.0493 uas:0.9887, las:0.9841\n",
            "epoch:12 step:3950 sesc:15.79s loss:0.0556 uas:0.9871, las:0.9821\n",
            "epoch:12 step:4000 sesc:15.83s loss:0.0509 uas:0.9881, las:0.9834\n",
            "epoch:12 step:4050 sesc:15.81s loss:0.0504 uas:0.9880, las:0.9836\n",
            "evaluation uas:0.9250 las:0.9109\n",
            "epoch:13 step:4100 sesc:17.89s loss:0.0425 uas:0.9903, las:0.9866\n",
            "epoch:13 step:4150 sesc:15.83s loss:0.0416 uas:0.9906, las:0.9867\n",
            "epoch:13 step:4200 sesc:15.85s loss:0.0432 uas:0.9899, las:0.9860\n",
            "epoch:13 step:4250 sesc:15.84s loss:0.0430 uas:0.9903, las:0.9862\n",
            "epoch:13 step:4300 sesc:15.85s loss:0.0455 uas:0.9894, las:0.9854\n",
            "epoch:13 step:4350 sesc:15.81s loss:0.0469 uas:0.9890, las:0.9851\n",
            "evaluation uas:0.9259 las:0.9107\n",
            "epoch:14 step:4400 sesc:17.90s loss:0.0418 uas:0.9900, las:0.9862\n",
            "epoch:14 step:4450 sesc:15.85s loss:0.0382 uas:0.9911, las:0.9878\n",
            "epoch:14 step:4500 sesc:15.82s loss:0.0421 uas:0.9903, las:0.9868\n",
            "epoch:14 step:4550 sesc:15.81s loss:0.0399 uas:0.9907, las:0.9871\n",
            "epoch:14 step:4600 sesc:15.78s loss:0.0417 uas:0.9904, las:0.9866\n",
            "epoch:14 step:4650 sesc:15.76s loss:0.0434 uas:0.9901, las:0.9862\n",
            "evaluation uas:0.9278 las:0.9127\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awlAseghE-EJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}