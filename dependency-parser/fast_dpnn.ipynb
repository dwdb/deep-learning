{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fast_dpnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1e_GKf3_9rUaRtgSPBknQYQ57h-W2dWZM",
      "authorship_tag": "ABX9TyPc7xPWbKe6JMtLlTIDT0Wd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dwdb/dependency-parser/blob/master/fast_dpnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYT5zZuMV9O8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "b9264177-df1b-414a-82fb-da121e4cd71a"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "np.random.seed(7)\n",
        "\n",
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Jun 19 07:29:18 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P8     7W /  75W |      0MiB /  7611MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaJUfn_MWHuq",
        "colab_type": "text"
      },
      "source": [
        "# 依存句法单词节点类\n",
        "节点id、词性、头节点id、左右孩子节点（指向的节点）、左右依存关系"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NC0n1fvYV-uf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Token(object):\n",
        "\n",
        "    def __init__(self, token_id, word, pos, dep, head_id):\n",
        "        self.token_id = token_id\n",
        "        self.word = word.lower()\n",
        "        self.pos = pos\n",
        "        if head_id >= token_id:\n",
        "            self.dep = 'L_' + dep\n",
        "        else:\n",
        "            self.dep = 'R_' + dep\n",
        "        self.head_id = head_id\n",
        "        self.left = []\n",
        "        self.right = []\n",
        "\n",
        "    def __repr__(self):\n",
        "        return 'Token(token_id={token_id},word={word},head_id={head_id})'.format(\n",
        "            **self.__dict__)\n",
        "\n",
        "\n",
        "ROOT_TOKEN = Token(-1, '<root>', '<root>', '<root>', -1)\n",
        "NULL_TOKEN = Token(-1, '<null>', '<null>', '<null>', -1)\n",
        "UNK_TOKEN = Token(-1, '<unk>', '<unk>', '<unk>', -1)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxK-Z8YGWtff",
        "colab_type": "text"
      },
      "source": [
        "# 依存句法transfer-reduce句子类\n",
        "节点、缓冲、栈，通过get_next_input可获取当前状态下的单词、磁性、依存关系共计18+18+12个特征"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9Tbr-9PWGZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sentence(object):\n",
        "    def __init__(self, tokens):\n",
        "        self.tokens = tokens\n",
        "        self.buff = tokens.copy()\n",
        "        self.stack = [ROOT_TOKEN]\n",
        "        self.deps = []\n",
        "\n",
        "    def update_by_action(self, action=None):\n",
        "        if action is None:\n",
        "            action = self.get_action()\n",
        "        # 转移\n",
        "        if action == 'shift':\n",
        "            self.stack.append(self.buff.pop(0))\n",
        "        # 左弧 stack1 <- stack0\n",
        "        elif action.startswith('L_'):\n",
        "            token = self.stack.pop(-2)\n",
        "            token.dep = action\n",
        "            self.deps.append((self.stack[-1].token_id, token.token_id, action))\n",
        "            self.binary_insert(self.stack[-1].left, token)\n",
        "        # 右弧 stack1 -> stack0\n",
        "        elif action.startswith('R_'):\n",
        "            token = self.stack.pop(-1)\n",
        "            token.dep = action\n",
        "            self.deps.append((self.stack[-1].token_id, token.token_id, action))\n",
        "            self.binary_insert(self.stack[-1].right, token)\n",
        "        else:\n",
        "            raise ValueError('unknown state!')\n",
        "        return action\n",
        "\n",
        "    def get_action(self):\n",
        "        if len(self.stack) < 2:\n",
        "            return 'shift'\n",
        "        t1, t0 = self.stack[-2:]\n",
        "        # left arc\n",
        "        if t1.head_id == t0.token_id:\n",
        "            return t1.dep\n",
        "        # right arc\n",
        "        if t0.head_id == t1.token_id:\n",
        "            if any(t0.token_id == t.head_id for t in self.buff):\n",
        "                return 'shift'\n",
        "            return t0.dep\n",
        "        return 'shift'\n",
        "\n",
        "    def get_next_input(self, word2id, pos2id, dep2id):\n",
        "        \"\"\"将conll dataset转为fast dependency parser dataset\n",
        "        18 features of words and pos tags:\n",
        "            The top 3 words on the stack and buffer:\n",
        "                s1, s2, s3, b1, b2, b3\n",
        "            The first and second leftmost/rightmost children of top 2 words on stack:\n",
        "                lc1(s1), rc1(s1), lc2(s1), rc2(s1), lc1(s2), rc1(s2), lc2(s2), rc2(s2)\n",
        "            The leftmost/rightmost of leftmost/rightmost of top 2 words on stack:\n",
        "                lc1(lc1(s1)), rc1(rc1(s1)), lc1(lc1(s2)), rc1(rc1(s2))\n",
        "        12 features of dependencies, that excluding those 6 (18-6=12) words on the stack/buffer\n",
        "        \"\"\"\n",
        "\n",
        "        def pad_tokens(tokens, maxlen):\n",
        "            tokens = tokens[:maxlen]\n",
        "            if len(tokens) < maxlen:\n",
        "                tokens += [NULL_TOKEN] * (maxlen - len(tokens))\n",
        "            return tokens\n",
        "\n",
        "        def get_children(token):\n",
        "            lc1, lc2 = pad_tokens(token.left, 2)\n",
        "            rc1, rc2 = pad_tokens(token.right, 2)\n",
        "            llc1, = pad_tokens(lc1.left, 1)\n",
        "            rrc1, = pad_tokens(rc1.right, 1)\n",
        "            return [lc1, rc1, lc2, rc2, llc1, rrc1]\n",
        "\n",
        "        # top 3 features on stack\n",
        "        s1, s2, s3 = pad_tokens(self.stack[-1::-1], 3)\n",
        "        # 18 features\n",
        "        tokens = [s1, s2, s3] + pad_tokens(self.buff, 3) + get_children(s1) + get_children(s2)\n",
        "        # word, pos tag and dependency indices\n",
        "        input_word = [word2id.get(token.word, word2id[UNK_TOKEN.word]) for token in tokens]\n",
        "        input_pos = [pos2id.get(token.pos, pos2id[UNK_TOKEN.pos]) for token in tokens]\n",
        "        input_dep = [dep2id.get(token.dep, dep2id[UNK_TOKEN.dep]) for token in tokens[6:]]\n",
        "        return input_word, input_pos, input_dep\n",
        "\n",
        "    @staticmethod\n",
        "    def binary_insert(array, value, key=lambda x: x.token_id):\n",
        "        start, end = 0, len(array) - 1\n",
        "        while start <= end:\n",
        "            mid = int((start + end) / 2)\n",
        "            if key(value) >= key(array[mid]):\n",
        "                start = mid + 1\n",
        "            else:\n",
        "                end = mid - 1\n",
        "        array.insert(start, value)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axuRWJxnXSHS",
        "colab_type": "text"
      },
      "source": [
        "# Conll数据集类"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SomeK2tBXScv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "76dfca31-4dbf-4f81-f923-4c05b3189b7f"
      },
      "source": [
        "class ConllDataset(object):\n",
        "\n",
        "    @staticmethod\n",
        "    def load(path):\n",
        "        with open(path, encoding='utf8') as f:\n",
        "            dataset, tokens = [], []\n",
        "            for line in f.readlines():\n",
        "                if line == '\\n':\n",
        "                    dataset.append(Sentence(tokens))\n",
        "                    tokens = []\n",
        "                else:\n",
        "                    line = line.strip().split('\\t')\n",
        "                    token = Token(int(line[0]) - 1, line[1], line[4], line[7], int(line[6]) - 1)\n",
        "                    tokens.append(token)\n",
        "            if tokens:\n",
        "                dataset.append(Sentence(tokens))\n",
        "        return dataset\n",
        "\n",
        "    def fit_transform(self, path, min_count=2, shuffle=True):\n",
        "        dataset = self.load(path)\n",
        "        # build vocabulary\n",
        "        vocab = defaultdict(int)\n",
        "        pos_tags, deps = set(), set()\n",
        "        for sentence in dataset:\n",
        "            for token in sentence.tokens:\n",
        "                vocab[token.word] += 1\n",
        "                pos_tags.add(token.pos)\n",
        "                deps.add(token.dep)\n",
        "\n",
        "        # create word dictionary\n",
        "        vocab = {k for k, v in vocab.items() if v >= min_count}\n",
        "        vocab.update((ROOT_TOKEN.word, NULL_TOKEN.word, UNK_TOKEN.word))\n",
        "        self.word2id = dict(zip(sorted(vocab), range(len(vocab))))\n",
        "        # create part-of-speech dictionary\n",
        "        pos_tags.update((ROOT_TOKEN.pos, NULL_TOKEN.pos, UNK_TOKEN.pos))\n",
        "        self.pos2id = dict(zip(sorted(pos_tags), range(len(pos_tags))))\n",
        "        # create dependency and labelsdictionary\n",
        "        deps.update((ROOT_TOKEN.dep, NULL_TOKEN.dep, UNK_TOKEN.dep))\n",
        "        labels = deps.copy() | {'shift', UNK_TOKEN.dep}\n",
        "        self.dep2id = dict(zip(sorted(deps), range(len(deps))))\n",
        "        self.id2label = sorted(labels)\n",
        "        self.label2id = dict(zip(self.id2label, range(len(self.id2label))))\n",
        "\n",
        "        self._fit = True\n",
        "        dataset = self.transform(path, dataset=dataset, shuffle=shuffle)\n",
        "        return dataset\n",
        "\n",
        "    def transform(self, path=None, dataset=None, shuffle=False):\n",
        "        if not dataset and path:\n",
        "            dataset = self.load(path)\n",
        "        assert getattr(self, '_fit', None), 'Model must be fit before transform!'\n",
        "\n",
        "        error_count = 0\n",
        "        inputs = []\n",
        "        for i, sentence in enumerate(dataset):\n",
        "            if i % 5000 == 0:\n",
        "                print('transforming at line %d' % i)\n",
        "            while len(sentence.stack) > 1 or sentence.buff:\n",
        "                input_word, input_pos, input_dep = sentence.get_next_input(\n",
        "                    self.word2id, self.pos2id, self.dep2id)\n",
        "                try:\n",
        "                    output = sentence.update_by_action()\n",
        "                    output = self.label2id.get(output, self.label2id[UNK_TOKEN.dep])\n",
        "                except (ValueError, IndexError) as e:\n",
        "                    error_count += 1\n",
        "                    break\n",
        "                inputs.append((input_word, input_pos, input_dep, output))\n",
        "\n",
        "        # shuffle dataset\n",
        "        if shuffle:\n",
        "            np.random.shuffle(inputs)\n",
        "\n",
        "        print('%s: error count:%d, total count:%d, total examples:%d' % (\n",
        "            os.path.basename(str(path)), error_count, len(dataset), len(inputs)))\n",
        "        # input_word, input_pos, input_dep, output\n",
        "        return tuple(np.array(data, np.int32) for data in zip(*inputs))\n",
        "\n",
        "\n",
        "conll_path = '/content/drive/My Drive/dependency parsing/data/conll'\n",
        "conll = ConllDataset()\n",
        "train_dataset = conll.fit_transform(os.path.join(conll_path, 'train.conll'))\n",
        "valid_dataset = conll.transform(os.path.join(conll_path, 'dev.conll'))\n",
        "\n",
        "output_path = './output/'\n",
        "if not os.path.exists(output_path):\n",
        "    os.mkdir(output_path)\n",
        "pickle.dump(conll.word2id, open(os.path.join(output_path, 'word2id.pkl'), 'wb'))\n",
        "pickle.dump(conll.pos2id, open(os.path.join(output_path, 'pos2id.pkl'), 'wb'))\n",
        "pickle.dump(conll.dep2id, open(os.path.join(output_path, 'dep2id.pkl'), 'wb'))\n",
        "pickle.dump(conll.label2id, open(os.path.join(output_path, 'label2id.pkl'), 'wb'))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "transforming at line 0\n",
            "transforming at line 5000\n",
            "transforming at line 10000\n",
            "transforming at line 15000\n",
            "transforming at line 20000\n",
            "transforming at line 25000\n",
            "transforming at line 30000\n",
            "transforming at line 35000\n",
            "train.conll: error count:120, total count:39832, total examples:1899368\n",
            "transforming at line 0\n",
            "dev.conll: error count:5, total count:1700, total examples:80201\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpTRzReiY9zs",
        "colab_type": "text"
      },
      "source": [
        "# 创建fast dependency model\n",
        "单词、词性、依存关系存于同一字典，只需创建一个embedding变量即可，但是这样做不好"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR-GAo9tYnhF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "c7c3f03b-b23c-4ae6-e593-4d4bccaa0dce"
      },
      "source": [
        "def build_model(vocab_size, pos_size, dep_size, embedding_size, n_classes):\n",
        "    print('vocab_size:%d, pos_size:%d, dep_size:%d, n_classes:%d' % (\n",
        "        vocab_size, pos_size, dep_size, n_classes))\n",
        "    l2_regularizer = tf.keras.regularizers.l2(1e-5)\n",
        "    # input layer\n",
        "    input_word = tf.keras.layers.Input(shape=(18,))\n",
        "    input_pos = tf.keras.layers.Input(shape=(18,))\n",
        "    input_dep = tf.keras.layers.Input(shape=(12,))\n",
        "    # embedding layer，initial weight range of [-0.01, 0.01]\n",
        "    word_embedding = tf.keras.layers.Embedding(\n",
        "        vocab_size, embedding_size,\n",
        "        embeddings_initializer=tf.keras.initializers.RandomUniform(-0.01, 0.01),\n",
        "        embeddings_regularizer=l2_regularizer)(input_word)\n",
        "    pos_embedding = tf.keras.layers.Embedding(\n",
        "        pos_size, embedding_size,\n",
        "        embeddings_initializer=tf.keras.initializers.RandomUniform(-0.01, 0.01),\n",
        "        embeddings_regularizer=l2_regularizer)(input_pos)\n",
        "    dep_embedding = tf.keras.layers.Embedding(\n",
        "        dep_size, embedding_size,\n",
        "        embeddings_initializer=tf.keras.initializers.RandomUniform(-0.01, 0.01),\n",
        "        embeddings_regularizer=l2_regularizer)(input_dep)\n",
        "    # shape=(batch_size, 48, embedding_size)\n",
        "    embedding = tf.concat((word_embedding, pos_embedding, dep_embedding), axis=1)\n",
        "    embedding = tf.reshape(embedding, shape=(-1, embedding_size * 48))\n",
        "    embedding = tf.keras.layers.Dropout(rate=0.4)(embedding)\n",
        "    # dense layer\n",
        "    dense1 = tf.keras.layers.Dense(\n",
        "        units=100,\n",
        "        kernel_regularizer=l2_regularizer,\n",
        "        bias_regularizer=l2_regularizer)(embedding)\n",
        "    dense1 = tf.pow(dense1, 3)\n",
        "    dense1 = tf.keras.layers.Dropout(rate=0.4)(dense1)\n",
        "    # dense layer\n",
        "    outputs = tf.keras.layers.Dense(\n",
        "        units=n_classes,\n",
        "        kernel_regularizer=l2_regularizer,\n",
        "        bias_regularizer=l2_regularizer)(dense1)\n",
        "    model = tf.keras.Model(inputs=(input_word, input_pos, input_dep), outputs=outputs)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "\n",
        "model = build_model(vocab_size=len(conll.word2id), pos_size=len(conll.pos2id), \n",
        "                    dep_size=len(conll.dep2id), embedding_size=50, n_classes=len(conll.label2id))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab_size:21679, pos_size:48, dep_size:73, n_classes:74\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            [(None, 18)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_8 (InputLayer)            [(None, 18)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_9 (InputLayer)            [(None, 12)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_6 (Embedding)         (None, 18, 50)       1083950     input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_7 (Embedding)         (None, 18, 50)       2400        input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_8 (Embedding)         (None, 12, 50)       3650        input_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2 (TensorFlo [(None, 48, 50)]     0           embedding_6[0][0]                \n",
            "                                                                 embedding_7[0][0]                \n",
            "                                                                 embedding_8[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape_2 (TensorFl [(None, 2400)]       0           tf_op_layer_concat_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 2400)         0           tf_op_layer_Reshape_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 100)          240100      dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Pow_2 (TensorFlowOp [(None, 100)]        0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 100)          0           tf_op_layer_Pow_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 74)           7474        dropout_5[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,337,574\n",
            "Trainable params: 1,337,574\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0SXPJgRtaxZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "1ca279cd-7780-49a2-b9de-abdf2ff050c8"
      },
      "source": [
        "history = model.fit(train_dataset[:3], train_dataset[3],\n",
        "    batch_size=2048, epochs=10, validation_data=(valid_dataset[:3], valid_dataset[3]))\n",
        "model.save(os.path.join(output_path, 'checkpoint'))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "928/928 [==============================] - 17s 19ms/step - loss: 0.2733 - accuracy: 0.9185 - val_loss: 0.1808 - val_accuracy: 0.9486\n",
            "Epoch 2/10\n",
            "928/928 [==============================] - 17s 18ms/step - loss: 0.2370 - accuracy: 0.9308 - val_loss: 0.1679 - val_accuracy: 0.9526\n",
            "Epoch 3/10\n",
            "928/928 [==============================] - 17s 18ms/step - loss: 0.2243 - accuracy: 0.9352 - val_loss: 0.1620 - val_accuracy: 0.9552\n",
            "Epoch 4/10\n",
            "928/928 [==============================] - 17s 18ms/step - loss: 0.2177 - accuracy: 0.9377 - val_loss: 0.1591 - val_accuracy: 0.9565\n",
            "Epoch 5/10\n",
            "928/928 [==============================] - 17s 18ms/step - loss: 0.2136 - accuracy: 0.9394 - val_loss: 0.1573 - val_accuracy: 0.9572\n",
            "Epoch 6/10\n",
            "928/928 [==============================] - 17s 18ms/step - loss: 0.2108 - accuracy: 0.9405 - val_loss: 0.1566 - val_accuracy: 0.9581\n",
            "Epoch 7/10\n",
            "928/928 [==============================] - 17s 18ms/step - loss: 0.2081 - accuracy: 0.9415 - val_loss: 0.1552 - val_accuracy: 0.9588\n",
            "Epoch 8/10\n",
            "928/928 [==============================] - 17s 18ms/step - loss: 0.2059 - accuracy: 0.9423 - val_loss: 0.1550 - val_accuracy: 0.9592\n",
            "Epoch 9/10\n",
            "928/928 [==============================] - 17s 18ms/step - loss: 0.2049 - accuracy: 0.9427 - val_loss: 0.1539 - val_accuracy: 0.9590\n",
            "Epoch 10/10\n",
            "928/928 [==============================] - 17s 18ms/step - loss: 0.2040 - accuracy: 0.9431 - val_loss: 0.1551 - val_accuracy: 0.9585\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Assets written to: ./output/checkpoint/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I95_moDHmggh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bd885c9a-6a99-4f64-fe2d-d7ef5c38e09e"
      },
      "source": [
        "def evaluate(sentences, output_path):\n",
        "    # restore model\n",
        "    model = tf.keras.models.load_model(os.path.join(output_path, 'checkpoint'))\n",
        "    # load local dictionary\n",
        "    label2id = pickle.load(open(os.path.join(output_path, 'label2id.pkl'), 'rb'))\n",
        "    word2id = pickle.load(open(os.path.join(output_path, 'word2id.pkl'), 'rb'))\n",
        "    pos2id = pickle.load(open(os.path.join(output_path, 'pos2id.pkl'), 'rb'))\n",
        "    dep2id = pickle.load(open(os.path.join(output_path, 'dep2id.pkl'), 'rb'))\n",
        "    id2label, _ = zip(*sorted(label2id.items(), key=lambda x:x[1]))\n",
        "\n",
        "    uas = las = count = 0\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        raw_deps = {}\n",
        "        for token in sentence.tokens:\n",
        "            raw_deps[(token.head_id, token.token_id)] = token.dep\n",
        "            token.dep = NULL_TOKEN.dep\n",
        "            token.head_id = -1\n",
        "        count += len(sentence.tokens)\n",
        "        while len(sentence.stack) > 1 or sentence.buff:\n",
        "            input_word, input_pos, input_dep = sentence.get_next_input(word2id, pos2id, dep2id)\n",
        "            input_word = np.array([input_word], dtype=np.int32)\n",
        "            input_pos = np.array([input_pos], dtype=np.int32)\n",
        "            input_dep = np.array([input_dep], dtype=np.int32)\n",
        "            output = model.predict((input_word, input_pos, input_dep))\n",
        "            action = id2label[np.argmax(output[0])]\n",
        "            try:\n",
        "                sentence.update_by_action(action)\n",
        "            except (IndexError, ValueError):\n",
        "                # print(len(sentence.tokens), len(sentence.deps))\n",
        "                break\n",
        "\n",
        "        for head, tail, action in sentence.deps:\n",
        "            raw_action = raw_deps.get((head, tail))\n",
        "            if raw_action is not None:\n",
        "                uas += 1\n",
        "                if raw_action == action:\n",
        "                    las += 1\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print('total sentence:%d, UAS:%.3f, LAS:%.3f' % (i + 1, uas / count, las / count))\n",
        "\n",
        "\n",
        "sentences = ConllDataset.load(os.path.join(conll_path, 'dev.conll'))\n",
        "evaluate(sentences, output_path)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total sentence:10, UAS:0.927, LAS:0.906\n",
            "total sentence:20, UAS:0.897, LAS:0.870\n",
            "total sentence:30, UAS:0.886, LAS:0.862\n",
            "total sentence:40, UAS:0.884, LAS:0.861\n",
            "total sentence:50, UAS:0.876, LAS:0.853\n",
            "total sentence:60, UAS:0.866, LAS:0.844\n",
            "total sentence:70, UAS:0.871, LAS:0.848\n",
            "total sentence:80, UAS:0.861, LAS:0.836\n",
            "total sentence:90, UAS:0.855, LAS:0.833\n",
            "total sentence:100, UAS:0.853, LAS:0.830\n",
            "total sentence:110, UAS:0.857, LAS:0.834\n",
            "total sentence:120, UAS:0.851, LAS:0.828\n",
            "total sentence:130, UAS:0.858, LAS:0.835\n",
            "total sentence:140, UAS:0.863, LAS:0.841\n",
            "total sentence:150, UAS:0.861, LAS:0.839\n",
            "total sentence:160, UAS:0.862, LAS:0.840\n",
            "total sentence:170, UAS:0.864, LAS:0.842\n",
            "total sentence:180, UAS:0.864, LAS:0.842\n",
            "total sentence:190, UAS:0.863, LAS:0.842\n",
            "total sentence:200, UAS:0.860, LAS:0.839\n",
            "total sentence:210, UAS:0.856, LAS:0.834\n",
            "total sentence:220, UAS:0.859, LAS:0.837\n",
            "total sentence:230, UAS:0.861, LAS:0.840\n",
            "total sentence:240, UAS:0.863, LAS:0.842\n",
            "total sentence:250, UAS:0.864, LAS:0.843\n",
            "total sentence:260, UAS:0.866, LAS:0.845\n",
            "total sentence:270, UAS:0.866, LAS:0.845\n",
            "total sentence:280, UAS:0.869, LAS:0.848\n",
            "total sentence:290, UAS:0.872, LAS:0.851\n",
            "total sentence:300, UAS:0.872, LAS:0.852\n",
            "total sentence:310, UAS:0.872, LAS:0.852\n",
            "total sentence:320, UAS:0.872, LAS:0.852\n",
            "total sentence:330, UAS:0.874, LAS:0.854\n",
            "total sentence:340, UAS:0.876, LAS:0.857\n",
            "total sentence:350, UAS:0.877, LAS:0.858\n",
            "total sentence:360, UAS:0.877, LAS:0.858\n",
            "total sentence:370, UAS:0.875, LAS:0.857\n",
            "total sentence:380, UAS:0.875, LAS:0.857\n",
            "total sentence:390, UAS:0.875, LAS:0.857\n",
            "total sentence:400, UAS:0.875, LAS:0.857\n",
            "total sentence:410, UAS:0.876, LAS:0.858\n",
            "total sentence:420, UAS:0.877, LAS:0.859\n",
            "total sentence:430, UAS:0.877, LAS:0.859\n",
            "total sentence:440, UAS:0.877, LAS:0.858\n",
            "total sentence:450, UAS:0.876, LAS:0.858\n",
            "total sentence:460, UAS:0.876, LAS:0.858\n",
            "total sentence:470, UAS:0.876, LAS:0.858\n",
            "total sentence:480, UAS:0.876, LAS:0.858\n",
            "total sentence:490, UAS:0.875, LAS:0.856\n",
            "total sentence:500, UAS:0.873, LAS:0.855\n",
            "total sentence:510, UAS:0.872, LAS:0.853\n",
            "total sentence:520, UAS:0.871, LAS:0.852\n",
            "total sentence:530, UAS:0.871, LAS:0.851\n",
            "total sentence:540, UAS:0.871, LAS:0.852\n",
            "total sentence:550, UAS:0.872, LAS:0.852\n",
            "total sentence:560, UAS:0.872, LAS:0.853\n",
            "total sentence:570, UAS:0.873, LAS:0.854\n",
            "total sentence:580, UAS:0.873, LAS:0.854\n",
            "total sentence:590, UAS:0.872, LAS:0.853\n",
            "total sentence:600, UAS:0.872, LAS:0.853\n",
            "total sentence:610, UAS:0.871, LAS:0.852\n",
            "total sentence:620, UAS:0.870, LAS:0.851\n",
            "total sentence:630, UAS:0.869, LAS:0.850\n",
            "total sentence:640, UAS:0.870, LAS:0.851\n",
            "total sentence:650, UAS:0.870, LAS:0.851\n",
            "total sentence:660, UAS:0.871, LAS:0.851\n",
            "total sentence:670, UAS:0.869, LAS:0.850\n",
            "total sentence:680, UAS:0.870, LAS:0.850\n",
            "total sentence:690, UAS:0.870, LAS:0.851\n",
            "total sentence:700, UAS:0.871, LAS:0.853\n",
            "total sentence:710, UAS:0.871, LAS:0.852\n",
            "total sentence:720, UAS:0.871, LAS:0.852\n",
            "total sentence:730, UAS:0.870, LAS:0.852\n",
            "total sentence:740, UAS:0.871, LAS:0.852\n",
            "total sentence:750, UAS:0.871, LAS:0.853\n",
            "total sentence:760, UAS:0.872, LAS:0.854\n",
            "total sentence:770, UAS:0.872, LAS:0.853\n",
            "total sentence:780, UAS:0.872, LAS:0.854\n",
            "total sentence:790, UAS:0.873, LAS:0.854\n",
            "total sentence:800, UAS:0.873, LAS:0.855\n",
            "total sentence:810, UAS:0.873, LAS:0.854\n",
            "total sentence:820, UAS:0.872, LAS:0.854\n",
            "total sentence:830, UAS:0.872, LAS:0.854\n",
            "total sentence:840, UAS:0.872, LAS:0.853\n",
            "total sentence:850, UAS:0.872, LAS:0.853\n",
            "total sentence:860, UAS:0.873, LAS:0.854\n",
            "total sentence:870, UAS:0.873, LAS:0.855\n",
            "total sentence:880, UAS:0.873, LAS:0.855\n",
            "total sentence:890, UAS:0.873, LAS:0.855\n",
            "total sentence:900, UAS:0.872, LAS:0.854\n",
            "total sentence:910, UAS:0.872, LAS:0.854\n",
            "total sentence:920, UAS:0.872, LAS:0.855\n",
            "total sentence:930, UAS:0.873, LAS:0.855\n",
            "total sentence:940, UAS:0.873, LAS:0.855\n",
            "total sentence:950, UAS:0.873, LAS:0.856\n",
            "total sentence:960, UAS:0.874, LAS:0.856\n",
            "total sentence:970, UAS:0.874, LAS:0.856\n",
            "total sentence:980, UAS:0.874, LAS:0.856\n",
            "total sentence:990, UAS:0.874, LAS:0.857\n",
            "total sentence:1000, UAS:0.874, LAS:0.857\n",
            "total sentence:1010, UAS:0.874, LAS:0.857\n",
            "total sentence:1020, UAS:0.873, LAS:0.855\n",
            "total sentence:1030, UAS:0.873, LAS:0.855\n",
            "total sentence:1040, UAS:0.873, LAS:0.855\n",
            "total sentence:1050, UAS:0.872, LAS:0.855\n",
            "total sentence:1060, UAS:0.872, LAS:0.855\n",
            "total sentence:1070, UAS:0.872, LAS:0.855\n",
            "total sentence:1080, UAS:0.873, LAS:0.856\n",
            "total sentence:1090, UAS:0.873, LAS:0.856\n",
            "total sentence:1100, UAS:0.873, LAS:0.856\n",
            "total sentence:1110, UAS:0.873, LAS:0.856\n",
            "total sentence:1120, UAS:0.873, LAS:0.855\n",
            "total sentence:1130, UAS:0.873, LAS:0.856\n",
            "total sentence:1140, UAS:0.873, LAS:0.856\n",
            "total sentence:1150, UAS:0.873, LAS:0.856\n",
            "total sentence:1160, UAS:0.873, LAS:0.856\n",
            "total sentence:1170, UAS:0.873, LAS:0.856\n",
            "total sentence:1180, UAS:0.873, LAS:0.856\n",
            "total sentence:1190, UAS:0.872, LAS:0.855\n",
            "total sentence:1200, UAS:0.873, LAS:0.855\n",
            "total sentence:1210, UAS:0.873, LAS:0.856\n",
            "total sentence:1220, UAS:0.873, LAS:0.856\n",
            "total sentence:1230, UAS:0.873, LAS:0.856\n",
            "total sentence:1240, UAS:0.873, LAS:0.856\n",
            "total sentence:1250, UAS:0.873, LAS:0.856\n",
            "total sentence:1260, UAS:0.874, LAS:0.856\n",
            "total sentence:1270, UAS:0.873, LAS:0.856\n",
            "total sentence:1280, UAS:0.873, LAS:0.855\n",
            "total sentence:1290, UAS:0.873, LAS:0.856\n",
            "total sentence:1300, UAS:0.874, LAS:0.857\n",
            "total sentence:1310, UAS:0.874, LAS:0.857\n",
            "total sentence:1320, UAS:0.875, LAS:0.858\n",
            "total sentence:1330, UAS:0.874, LAS:0.857\n",
            "total sentence:1340, UAS:0.874, LAS:0.857\n",
            "total sentence:1350, UAS:0.875, LAS:0.858\n",
            "total sentence:1360, UAS:0.874, LAS:0.857\n",
            "total sentence:1370, UAS:0.874, LAS:0.857\n",
            "total sentence:1380, UAS:0.874, LAS:0.857\n",
            "total sentence:1390, UAS:0.873, LAS:0.857\n",
            "total sentence:1400, UAS:0.874, LAS:0.857\n",
            "total sentence:1410, UAS:0.873, LAS:0.857\n",
            "total sentence:1420, UAS:0.873, LAS:0.857\n",
            "total sentence:1430, UAS:0.873, LAS:0.857\n",
            "total sentence:1440, UAS:0.874, LAS:0.857\n",
            "total sentence:1450, UAS:0.874, LAS:0.857\n",
            "total sentence:1460, UAS:0.874, LAS:0.858\n",
            "total sentence:1470, UAS:0.875, LAS:0.858\n",
            "total sentence:1480, UAS:0.875, LAS:0.858\n",
            "total sentence:1490, UAS:0.875, LAS:0.858\n",
            "total sentence:1500, UAS:0.875, LAS:0.858\n",
            "total sentence:1510, UAS:0.875, LAS:0.858\n",
            "total sentence:1520, UAS:0.875, LAS:0.858\n",
            "total sentence:1530, UAS:0.874, LAS:0.858\n",
            "total sentence:1540, UAS:0.874, LAS:0.858\n",
            "total sentence:1550, UAS:0.874, LAS:0.857\n",
            "total sentence:1560, UAS:0.874, LAS:0.857\n",
            "total sentence:1570, UAS:0.873, LAS:0.857\n",
            "total sentence:1580, UAS:0.873, LAS:0.856\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-628dc30c2ade>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConllDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconll_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dev.conll'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-628dc30c2ade>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(sentences, output_path)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0minput_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_pos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0minput_dep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_dep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid2label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m     87\u001b[0m           method.__name__))\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1247\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m           model=self)\n\u001b[0m\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;31m# trigger the next permutation. On the other hand, too many simultaneous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;31m# shuffles can contend on a hardware level and degrade all performance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1619\u001b[0m     \"\"\"\n\u001b[1;32m   1620\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1621\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1622\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m       return ParallelMapDataset(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   3979\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3980\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3981\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   3982\u001b[0m     variant_tensor = gen_dataset_ops.map_dataset(\n\u001b[1;32m   3983\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3219\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3220\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3221\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3223\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2530\u001b[0m     \"\"\"\n\u001b[1;32m   2531\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 2532\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   2533\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2534\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2494\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2495\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2496\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2497\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2498\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2667\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3212\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   3213\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3214\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3215\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3154\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3156\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3157\u001b[0m       \u001b[0;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3158\u001b[0m       \u001b[0;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_whitelisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mpermutation\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m    317\u001b[0m       \u001b[0;31m# than reusing the same range Tensor. (presumably because of buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m       \u001b[0;31m# forwarding.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m       \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_shuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mrange\u001b[0;34m(start, limit, delta, dtype, name)\u001b[0m\n\u001b[1;32m   1590\u001b[0m     \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1592\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_range\u001b[0;34m(start, limit, delta, name)\u001b[0m\n\u001b[1;32m   7122\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7123\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0;32m-> 7124\u001b[0;31m         \"Range\", start=start, limit=limit, delta=delta, name=name)\n\u001b[0m\u001b[1;32m   7125\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7126\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    351\u001b[0m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m   \u001b[0minput_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;31m# Perform input type inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36minner_cm\u001b[0;34m()\u001b[0m\n\u001b[1;32m    392\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         if (uses_distribution_strategy or\n\u001b[0;32m--> 394\u001b[0;31m             device_stack_has_callable(graph._device_function_stack)):\n\u001b[0m\u001b[1;32m    395\u001b[0m           \u001b[0;31m# Hard-code devices from device functions in the function body\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device_function_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device_function_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mdevice_stack_has_callable\u001b[0;34m(device_stack)\u001b[0m\n\u001b[1;32m   1049\u001b[0m   \u001b[0;34m\"\"\"Checks whether a device stack contains a callable.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m   return any(callable(spec._device_name_or_function)  # pylint: disable=protected-access\n\u001b[0;32m-> 1051\u001b[0;31m              for spec in device_stack.peek_objs())\n\u001b[0m\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/traceable_stack.py\u001b[0m in \u001b[0;36mpeek_objs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpeek_objs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;34m\"\"\"Return iterator over stored objects ordered newest to oldest.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt_obj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpeek_traceable_objs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}